# SKCC_LCL
# DoLa : Decoding by Contrasting Layers Improves Factuality in Large Language Models

<aside>
ğŸ’¡ LLMì˜ í™˜ê°ì„ ì¤„ì´ê¸° ìœ„í•œ ìƒˆë¡œìš´ ë””ì½”ë”© ì „ëµ

</aside>

# Introduction

- LLMì˜ í™˜ê°
    
    LLMì€ **ì‚¬ì „ í•™ìŠµì—ì„œ ê´€ì°°ëœ ì‹¤ì œ ì‚¬ì‹¤ì—ì„œ ë²—ì–´ë‚œ ë‚´ìš©ì„ ìƒì„±**í•˜ëŠ” â€œí™˜ê°â€ì„ ì¼ìœ¼í‚¤ëŠ” ê²½í–¥ì´ ìˆìŒ. 
    
    í™˜ê°ì˜ ì´ìœ ëŠ” ëª…í™•í•˜ì§€ ì•Šìœ¼ë‚˜, ë°ì´í„°ì™€ ì–¸ì–´ëª¨ë¸ì˜ í™•ë¥  ë¶„í¬ ê°„ ì°¨ì´(KL-divergence)ë¥¼ ì¤„ì´ê³ ì í•™ìŠµí•˜ëŠ” **language modeling objective** ê°€ ì›ì¸ì´ ë  ìˆ˜ ìˆìŒ. ì¦‰, ì–¸ì–´ ëª¨ë¸ì€ í•™ìŠµëœ ì½”í¼ìŠ¤ì—ì„œ ì‹¤ì œ ì‚¬ì‹¤ì„ ì¸ì‹í•˜ëŠ” ëŒ€ì‹ ì— ì™¸ë¶€ íŒ¨í„´ì„ ì¸ì‹í•˜ë©° í•™ìŠµë¨. 
    
- ëª¨ë¸ í•´ì„ ê´€ì 
    
    ![Alt text](image.png)
    
    ì´ì „ ì—°êµ¬ì—ì„œ LMì˜ í•˜ìœ„ ë ˆì´ì–´ëŠ” lower-level information(ì€,ëŠ”,ì´,ê°€,í•œ,â€¦)ì„, ìƒìœ„ ë ˆì´ì–´ëŠ” semantic information(6.25ì „ìŸ, ë°œìƒ, ì—°ë„,â€¦)ì„ ì¸ì½”ë”©í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¨. 
    
    ë˜í•œ LM ë‚´ì—ì„œ íŠ¹ì • feed-forward layerë¥¼ ë³€í˜•í•˜ì—¬ ì‚¬ì‹¤ì  ì§€ì‹ì„ í¸ì§‘í•  ìˆ˜ ìˆìŒì„ ë³´ì—¬ì¤Œ. 
    

# DoLa
![Alt text](image-1.png)

ìœ„ ê·¸ë¦¼ì—ì„œ LMì€ ë§ˆì§€ë§‰ layerì—ì„œ ë†’ì€ í™•ë¥ ì„ ê°€ì§€ëŠ” Seattleì„ outputìœ¼ë¡œ ë±‰ê²Œ ë¨. 

ì‹¤ì œ ì •ë‹µ(Olympia)ì€ ìƒìœ„ ë ˆì´ì–´ ì¼ìˆ˜ë¡ í™•ë¥ ì´ ì¦ê°€í•´ì•¼ë˜ëŠ”ë°, í•˜ìœ„ ë ˆì´ì–´ì—ì„œë¶€í„° í™•ë¥ ì´ ë†’ì•˜ë˜ ê²ƒìœ¼ë¡œ ì¸í•˜ì—¬ í™˜ê°ì´ ë°œìƒë¨. 

â†’ ì„œë¡œ ë‹¤ë¥¸ ë ˆì´ì–´ ê°„ì˜ í™•ë¥  ë¶„í¬ë¥¼ ëŒ€ì¡°í•˜ì—¬ ì´ë¥¼ êµì •í•˜ë©´ í™˜ê°ì„ ì¤„ì¼ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œ?

![Alt text](image-2.png)

ìœ„ ê·¸ë¦¼ì€ ë§ˆì§€ë§‰ ë ˆì´ì–´ì™€ ì´ì „ ë ˆì´ì–´ë“¤ì˜ JSD(Jenson-Shannon divergence)ë¥¼ ê³„ì‚°í•œ ê²ƒì„. JSDê°€ ì‘ì„ ìˆ˜ë¡ ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ë¶„í¬ì™€ ì°¨ì´ê°€ ì ë‹¤ëŠ” ì˜ë¯¸ë¥¼ ì§€ë‹˜. 

1) JSDê°€ ì¤‘ìš”í•œ ì—”í‹°í‹°ì˜ ìƒìœ„ ë ˆì´ì–´ì—ì„œ ì—¬ì „íˆ ë†’ìŒ. â†’ ì´ëŠ” ë§ˆì§€ë§‰ ë ˆì´ì–´ì™€ ì°¨ì´ê°€ í° ê²ƒì„ ì˜ë¯¸í•˜ê³  ì˜ˆì¸¡ì„ ë³€ê²½í•  ìˆ˜ ìˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ì˜ë¯¸í•¨.

2) ì‰¬ìš´ í† í°ì„ ì˜ˆì¸¡í•  ë•Œ ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ë§¤ìš° ì‘ì•„ì§. â†’ ëª¨ë¸ì´ ì¤‘ê°„ ë ˆì´ì–´ì—ì„œ ìƒì„±í•  í† í°ì„ ì´ë¯¸ ê²°ì •í–ˆìœ¼ë©° ì¶œë ¥ ë¶„í¬ë¥¼ ê±°ì˜ ë³€ê²½í•˜ì§€ ì•Šê³  ìœ ì§€í•¨. 

â†’ ë ˆì´ì–´ì˜ JSDê°€ ê°‘ìê¸° ë³€í•  ë•Œë¥¼ ëŒ€ì¡°í•˜ë©´(contrastive decoding), LMì˜ ì‹¤ì œ ì‚¬ì‹¤ì„ ì¦í­ì‹œì¼œ ì‚¬ì‹¤ì  ì§€ì‹ì„ íš¨ê³¼ì ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ì˜ëª»ëœ ì‚¬ì‹¤ì„ ìƒì„±í•˜ëŠ” í™˜ê° í˜„ìƒì„ í•´ì†Œí•  ìˆ˜ ìˆì„ ê²ƒì„!

ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ì¶”ê°€ì ì¸ 1) ì™¸ë¶€ì§€ì‹ì´ í•„ìš”í•˜ì§€ ì•Šê³  2) ì–´ë– í•œ íŒŒì¸ íŠœë‹ë„ í•˜ì§€ ì•Šì•„ë„ ëœë‹¤ëŠ” ì¥ì ì„ ê°€ì§.


# Method

(1) transformer ì˜ ë§ˆì§€ë§‰ layerì™€ ë‹¤ë¥¸ layer ê°„ì˜ JSDë¥¼ ê³„ì‚°í•˜ì—¬ ë¶„í¬ê°€ ê°€ì¥ í¬ê²Œ ë³€í™”í•˜ëŠ” ë ˆì´ì–´ë¥¼ ì°¾ìŒ. 

(2) (1)ì—ì„œ ì°¾ì€ layerì˜ logitê³¼ ì›ë˜ transformer ë§ˆì§€ë§‰ layerì˜ logitì„ ë¹„êµí•˜ì—¬, ìµœì¢… logitì„ êµì •í•¨. 

- ì½”ë“œ
    
    ```python
    # 1. Stacking all premature_layers into a new dimension
    stacked_premature_layers = torch.stack(
        [candidate_premature_logits[i].to(final_logits) for i in candidate_premature_layers], dim=0
    )
    # 2. Calculate the softmax values for mature_layer and all premature_layers
    softmax_mature_layer = F.softmax(final_logits, dim=-1).detach() # shape: (batch_size, vocab_size)
    softmax_premature_layers = F.softmax(
        stacked_premature_layers, dim=-1
    )  # shape: (num_premature_layers, batch_size, vocab_size)
    
    # 3. Calculate M, the average distribution
    M = 0.5 * (
        softmax_mature_layer[None, :, :] + softmax_premature_layers
    )  # shape: (num_premature_layers, batch_size, vocab_size)
    
    # 4. Calculate log-softmax for the KL divergence
    log_softmax_mature_layer = F.log_softmax(final_logits, dim=-1)  # shape: (batch_size, vocab_size)
    log_softmax_premature_layers = F.log_softmax(
        stacked_premature_layers, dim=-1
    )  # shape: (num_premature_layers, batch_size, vocab_size)
    
    # 5. Calculate the KL divergences and then the JS divergences
    kl1 = F.kl_div(log_softmax_mature_layer[None, :, :], M, reduction="none").mean(
        -1
    )  # shape: (num_premature_layers, batch_size)
    kl2 = F.kl_div(log_softmax_premature_layers, M, reduction="none").mean(
        -1
    )  # shape: (num_premature_layers, batch_size)
    js_divs = 0.5 * (kl1 + kl2)  # shape: (num_premature_layers, batch_size)
    # 6. Reduce the batchmean
    js_divs = js_divs.mean(-1)  # shape: (num_premature_layers,)
    
    premature_layer = candidate_premature_layers[int(js_divs.argmax().cpu().item())]
    base_logits = candidate_premature_logits[premature_layer]
    final_logits, base_logits = _relative_top_filter(final_logits.detach(), base_logits.detach())
    logits = final_logits - base_logits.to(final_logits.device)
    ```
    
    ```python
    def _relative_top_filter(
        scores: torch.FloatTensor,
        baseline_scores: torch.FloatTensor,
        relative_top: float = 0.1,
        filter_value: float = -float("Inf"),
        base_filter_value=-1e-3,
        min_tokens_to_keep: int = 1,
    ) -> torch.FloatTensor:
    
        scores_normalized = scores.log_softmax(dim=-1)
        baseline_scores_normalized = baseline_scores.log_softmax(dim=-1)
        sorted_logits, sorted_indices = torch.sort(scores_normalized, descending=True)
        min_thresh = sorted_logits[..., min_tokens_to_keep - 1]
        probs_max = torch.max(scores_normalized, dim=-1).values # ì‹¤ì œ ì •ë‹µ logit
        probs_thresh = probs_max + np.log(relative_top) # ì‹¤ì œ ì •ë‹µ logitì— ì–´ë–¤ ê°’ ë”í•œ ê²ƒ. 
        probs_thresh = torch.min(min_thresh, probs_thresh) 
        probs_thresh = probs_thresh.unsqueeze(-1)
        baseline_scores_normalized[scores_normalized < probs_thresh] = base_filter_value
        scores_normalized[scores_normalized < probs_thresh] = filter_value
        return scores_normalized, baseline_scores_normalized
    
    ```